{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Root Cause Analysis with Controlled Anomaly Injection\n",
    "\n",
    "## Objective\n",
    "This notebook demonstrates the root cause analysis capabilities of the Intervention Search system by:\n",
    "\n",
    "1. **Creating a realistic synthetic dataset** with a known causal structure\n",
    "2. **Injecting controlled anomalies** at specific nodes in the causal graph\n",
    "3. **Propagating anomaly effects** through the causal DAG to downstream nodes\n",
    "4. **Training models using Auto ML mode** - testing multiple model types per node\n",
    "5. **Testing detection capabilities** - verifying if the system can identify the true root causes\n",
    "\n",
    "## Causal Structure: E-commerce Platform\n",
    "\n",
    "We'll simulate an e-commerce platform with the following causal structure:\n",
    "\n",
    "```\n",
    "Marketing Spend → Website Traffic → Conversion Rate → Orders → Revenue\n",
    "                                           ↑            ↑\n",
    "                                    Page Load Time    |\n",
    "                                           ↑            |\n",
    "                                    Server Capacity    |\n",
    "                                                        |\n",
    "Inventory Level ----------------------------------------+\n",
    "                    ↓\n",
    "               Stock-out Rate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Import HT RCA tools\n",
    "from ht_categ import HT, HTConfig\n",
    "\n",
    "print(\"✅ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Causal Graph Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DEFINE CAUSAL DAG\n",
    "# ============================================================================\n",
    "\n",
    "edges = [\n",
    "    # Marketing & Traffic\n",
    "    ('marketing_spend', 'website_traffic'),\n",
    "    \n",
    "    # Infrastructure\n",
    "    ('server_capacity', 'page_load_time'),\n",
    "    ('website_traffic', 'page_load_time'),  # More traffic = slower pages\n",
    "    \n",
    "    # Conversion funnel\n",
    "    ('website_traffic', 'conversion_rate'),\n",
    "    ('page_load_time', 'conversion_rate'),  # Slow pages reduce conversion\n",
    "    \n",
    "    # Orders\n",
    "    ('website_traffic', 'orders'),\n",
    "    ('conversion_rate', 'orders'),\n",
    "    ('inventory_level', 'orders'),  # Low inventory limits orders\n",
    "    \n",
    "    # Stock management\n",
    "    ('inventory_level', 'stockout_rate'),\n",
    "    \n",
    "    # Revenue\n",
    "    ('orders', 'revenue'),\n",
    "    ('stockout_rate', 'revenue'),  # Stock-outs hurt revenue\n",
    "]\n",
    "\n",
    "# Create adjacency matrix\n",
    "all_nodes = sorted(set([node for edge in edges for node in edge]))\n",
    "adj_matrix = pd.DataFrame(0, index=all_nodes, columns=all_nodes)\n",
    "\n",
    "for source, target in edges:\n",
    "    adj_matrix.loc[source, target] = 1\n",
    "\n",
    "print(\"Causal Graph:\")\n",
    "print(f\"  Nodes: {len(all_nodes)}\")\n",
    "print(f\"  Edges: {len(edges)}\")\n",
    "print(f\"\\nNodes: {all_nodes}\")\n",
    "\n",
    "# Validate DAG\n",
    "G = nx.from_pandas_adjacency(adj_matrix, create_using=nx.DiGraph())\n",
    "assert nx.is_directed_acyclic_graph(G), \"Graph must be a DAG!\"\n",
    "print(\"\\n✅ Valid DAG structure confirmed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Realistic Baseline Data\n",
    "\n",
    "We'll generate data following the causal relationships with realistic noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA GENERATION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def generate_ecommerce_data(n_samples=1000, inject_anomaly=False, anomaly_config=None):\n",
    "    \"\"\"\n",
    "    Generate realistic e-commerce data following the causal DAG.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of data points to generate\n",
    "        inject_anomaly: Whether to inject anomalies\n",
    "        anomaly_config: Dictionary with anomaly specifications:\n",
    "            {\n",
    "                'node': str,  # Which node to inject anomaly in\n",
    "                'start_idx': int,  # When to start anomaly\n",
    "                'end_idx': int,  # When to end anomaly\n",
    "                'effect': float,  # Multiplicative effect (e.g., 0.5 = 50% reduction)\n",
    "                'description': str  # What went wrong\n",
    "            }\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with generated data\n",
    "    \"\"\"\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    # ROOT NODES (Exogenous variables)\n",
    "    # Marketing spend: $10K-$50K per period\n",
    "    data['marketing_spend'] = np.random.uniform(10000, 50000, n_samples)\n",
    "    \n",
    "    # Server capacity: 100-500 units\n",
    "    data['server_capacity'] = np.random.uniform(100, 500, n_samples)\n",
    "    \n",
    "    # Inventory level: 1000-10000 units\n",
    "    data['inventory_level'] = np.random.uniform(1000, 10000, n_samples)\n",
    "    \n",
    "    # INTERMEDIATE NODES\n",
    "    # Website traffic: Driven by marketing spend\n",
    "    data['website_traffic'] = (\n",
    "        500 + \n",
    "        0.8 * data['marketing_spend'] + \n",
    "        np.random.normal(0, 2000, n_samples)\n",
    "    )\n",
    "    data['website_traffic'] = np.maximum(data['website_traffic'], 100)  # Floor at 100\n",
    "    \n",
    "    # Page load time: Affected by server capacity and traffic\n",
    "    # Higher traffic and lower capacity = slower pages\n",
    "    data['page_load_time'] = (\n",
    "        0.5 + \n",
    "        0.00002 * data['website_traffic'] - \n",
    "        0.002 * data['server_capacity'] + \n",
    "        np.random.normal(0, 0.1, n_samples)\n",
    "    )\n",
    "    data['page_load_time'] = np.maximum(data['page_load_time'], 0.1)  # Min 0.1s\n",
    "    \n",
    "    # Conversion rate: Traffic brings visitors, but slow pages hurt conversion\n",
    "    base_conversion = 0.03  # 3% base conversion\n",
    "    traffic_effect = 0.000001 * data['website_traffic']  # Slight positive effect\n",
    "    speed_penalty = -0.01 * data['page_load_time']  # Slow pages hurt\n",
    "    \n",
    "    data['conversion_rate'] = (\n",
    "        base_conversion + \n",
    "        traffic_effect + \n",
    "        speed_penalty + \n",
    "        np.random.normal(0, 0.005, n_samples)\n",
    "    )\n",
    "    data['conversion_rate'] = np.clip(data['conversion_rate'], 0.001, 0.1)  # 0.1% - 10%\n",
    "    \n",
    "    # Stock-out rate: Inversely related to inventory\n",
    "    data['stockout_rate'] = (\n",
    "        0.5 - \n",
    "        0.00004 * data['inventory_level'] + \n",
    "        np.random.normal(0, 0.05, n_samples)\n",
    "    )\n",
    "    data['stockout_rate'] = np.clip(data['stockout_rate'], 0, 0.5)  # 0-50%\n",
    "    \n",
    "    # Orders: Traffic * Conversion rate, limited by inventory\n",
    "    potential_orders = data['website_traffic'] * data['conversion_rate']\n",
    "    inventory_limit = data['inventory_level'] * 0.1  # Can sell 10% of inventory per period\n",
    "    \n",
    "    data['orders'] = np.minimum(\n",
    "        potential_orders + np.random.normal(0, 10, n_samples),\n",
    "        inventory_limit\n",
    "    )\n",
    "    data['orders'] = np.maximum(data['orders'], 0)\n",
    "    \n",
    "    # Revenue: Orders * average order value, reduced by stock-outs\n",
    "    avg_order_value = 100  # $100 per order\n",
    "    data['revenue'] = (\n",
    "        data['orders'] * avg_order_value * (1 - 0.3 * data['stockout_rate']) +\n",
    "        np.random.normal(0, 1000, n_samples)\n",
    "    )\n",
    "    data['revenue'] = np.maximum(data['revenue'], 0)\n",
    "    \n",
    "    # INJECT ANOMALY if requested\n",
    "    anomaly_info = None\n",
    "    if inject_anomaly and anomaly_config:\n",
    "        node = anomaly_config['node']\n",
    "        start_idx = anomaly_config['start_idx']\n",
    "        end_idx = anomaly_config['end_idx']\n",
    "        effect = anomaly_config['effect']\n",
    "        \n",
    "        if node in data:\n",
    "            # Store original values\n",
    "            original_mean = np.mean(data[node][start_idx:end_idx])\n",
    "            \n",
    "            # Apply multiplicative effect\n",
    "            data[node][start_idx:end_idx] *= effect\n",
    "            \n",
    "            # Re-propagate through the DAG\n",
    "            # This is crucial: anomalies cascade through causal relationships!\n",
    "            \n",
    "            if node == 'server_capacity':\n",
    "                # Recalculate downstream effects\n",
    "                data['page_load_time'][start_idx:end_idx] = (\n",
    "                    0.5 + \n",
    "                    0.00002 * data['website_traffic'][start_idx:end_idx] - \n",
    "                    0.002 * data['server_capacity'][start_idx:end_idx] + \n",
    "                    np.random.normal(0, 0.1, end_idx - start_idx)\n",
    "                )\n",
    "                data['page_load_time'][start_idx:end_idx] = np.maximum(\n",
    "                    data['page_load_time'][start_idx:end_idx], 0.1\n",
    "                )\n",
    "                \n",
    "                # Recalculate conversion rate\n",
    "                traffic_effect = 0.000001 * data['website_traffic'][start_idx:end_idx]\n",
    "                speed_penalty = -0.01 * data['page_load_time'][start_idx:end_idx]\n",
    "                data['conversion_rate'][start_idx:end_idx] = np.clip(\n",
    "                    base_conversion + traffic_effect + speed_penalty + \n",
    "                    np.random.normal(0, 0.005, end_idx - start_idx),\n",
    "                    0.001, 0.1\n",
    "                )\n",
    "                \n",
    "                # Recalculate orders\n",
    "                potential_orders = (\n",
    "                    data['website_traffic'][start_idx:end_idx] * \n",
    "                    data['conversion_rate'][start_idx:end_idx]\n",
    "                )\n",
    "                inventory_limit = data['inventory_level'][start_idx:end_idx] * 0.1\n",
    "                data['orders'][start_idx:end_idx] = np.maximum(\n",
    "                    np.minimum(\n",
    "                        potential_orders + np.random.normal(0, 10, end_idx - start_idx),\n",
    "                        inventory_limit\n",
    "                    ),\n",
    "                    0\n",
    "                )\n",
    "                \n",
    "                # Recalculate revenue\n",
    "                data['revenue'][start_idx:end_idx] = np.maximum(\n",
    "                    data['orders'][start_idx:end_idx] * avg_order_value * \n",
    "                    (1 - 0.3 * data['stockout_rate'][start_idx:end_idx]) +\n",
    "                    np.random.normal(0, 1000, end_idx - start_idx),\n",
    "                    0\n",
    "                )\n",
    "            \n",
    "            elif node == 'inventory_level':\n",
    "                # Recalculate stock-out rate\n",
    "                data['stockout_rate'][start_idx:end_idx] = np.clip(\n",
    "                    0.5 - 0.00004 * data['inventory_level'][start_idx:end_idx] + \n",
    "                    np.random.normal(0, 0.05, end_idx - start_idx),\n",
    "                    0, 0.5\n",
    "                )\n",
    "                \n",
    "                # Recalculate orders (limited by inventory)\n",
    "                potential_orders = (\n",
    "                    data['website_traffic'][start_idx:end_idx] * \n",
    "                    data['conversion_rate'][start_idx:end_idx]\n",
    "                )\n",
    "                inventory_limit = data['inventory_level'][start_idx:end_idx] * 0.1\n",
    "                data['orders'][start_idx:end_idx] = np.maximum(\n",
    "                    np.minimum(\n",
    "                        potential_orders + np.random.normal(0, 10, end_idx - start_idx),\n",
    "                        inventory_limit\n",
    "                    ),\n",
    "                    0\n",
    "                )\n",
    "                \n",
    "                # Recalculate revenue\n",
    "                data['revenue'][start_idx:end_idx] = np.maximum(\n",
    "                    data['orders'][start_idx:end_idx] * avg_order_value * \n",
    "                    (1 - 0.3 * data['stockout_rate'][start_idx:end_idx]) +\n",
    "                    np.random.normal(0, 1000, end_idx - start_idx),\n",
    "                    0\n",
    "                )\n",
    "            \n",
    "            elif node == 'marketing_spend':\n",
    "                # Recalculate website traffic\n",
    "                data['website_traffic'][start_idx:end_idx] = np.maximum(\n",
    "                    500 + 0.8 * data['marketing_spend'][start_idx:end_idx] + \n",
    "                    np.random.normal(0, 2000, end_idx - start_idx),\n",
    "                    100\n",
    "                )\n",
    "                \n",
    "                # Cascade through page load time, conversion, orders, revenue\n",
    "                data['page_load_time'][start_idx:end_idx] = np.maximum(\n",
    "                    0.5 + 0.00002 * data['website_traffic'][start_idx:end_idx] - \n",
    "                    0.002 * data['server_capacity'][start_idx:end_idx] + \n",
    "                    np.random.normal(0, 0.1, end_idx - start_idx),\n",
    "                    0.1\n",
    "                )\n",
    "                \n",
    "                traffic_effect = 0.000001 * data['website_traffic'][start_idx:end_idx]\n",
    "                speed_penalty = -0.01 * data['page_load_time'][start_idx:end_idx]\n",
    "                data['conversion_rate'][start_idx:end_idx] = np.clip(\n",
    "                    base_conversion + traffic_effect + speed_penalty + \n",
    "                    np.random.normal(0, 0.005, end_idx - start_idx),\n",
    "                    0.001, 0.1\n",
    "                )\n",
    "                \n",
    "                potential_orders = (\n",
    "                    data['website_traffic'][start_idx:end_idx] * \n",
    "                    data['conversion_rate'][start_idx:end_idx]\n",
    "                )\n",
    "                inventory_limit = data['inventory_level'][start_idx:end_idx] * 0.1\n",
    "                data['orders'][start_idx:end_idx] = np.maximum(\n",
    "                    np.minimum(\n",
    "                        potential_orders + np.random.normal(0, 10, end_idx - start_idx),\n",
    "                        inventory_limit\n",
    "                    ),\n",
    "                    0\n",
    "                )\n",
    "                \n",
    "                data['revenue'][start_idx:end_idx] = np.maximum(\n",
    "                    data['orders'][start_idx:end_idx] * avg_order_value * \n",
    "                    (1 - 0.3 * data['stockout_rate'][start_idx:end_idx]) +\n",
    "                    np.random.normal(0, 1000, end_idx - start_idx),\n",
    "                    0\n",
    "                )\n",
    "            \n",
    "            anomaly_mean = np.mean(data[node][start_idx:end_idx])\n",
    "            \n",
    "            anomaly_info = {\n",
    "                'node': node,\n",
    "                'start_idx': start_idx,\n",
    "                'end_idx': end_idx,\n",
    "                'effect': effect,\n",
    "                'original_mean': original_mean,\n",
    "                'anomaly_mean': anomaly_mean,\n",
    "                'pct_change': ((anomaly_mean - original_mean) / original_mean) * 100,\n",
    "                'description': anomaly_config.get('description', 'Unknown anomaly')\n",
    "            }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add time index\n",
    "    df['time_period'] = range(len(df))\n",
    "    \n",
    "    return df, anomaly_info\n",
    "\n",
    "print(\"✅ Data generation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Normal (Baseline) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GENERATE BASELINE DATA\n",
    "# ============================================================================\n",
    "\n",
    "df_normal, _ = generate_ecommerce_data(n_samples=800, inject_anomaly=False)\n",
    "\n",
    "print(f\"Normal data shape: {df_normal.shape}\")\n",
    "print(f\"\\nBaseline statistics:\")\n",
    "print(df_normal[all_nodes].describe())\n",
    "\n",
    "# Visualize baseline data\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, node in enumerate(all_nodes):\n",
    "    axes[idx].hist(df_normal[node], bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[idx].set_title(f'{node}\\n(μ={df_normal[node].mean():.1f}, σ={df_normal[node].std():.1f})')\n",
    "    axes[idx].set_xlabel('Value')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Baseline Data Distribution', y=1.02, fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Baseline data generated and visualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Anomalous Data with Known Root Cause\n",
    "\n",
    "We'll inject **THREE different anomalies** to test the detection system:\n",
    "\n",
    "1. **Server Capacity Reduction** (50% reduction) - Simulates infrastructure failure\n",
    "2. **Inventory Shortage** (30% reduction) - Simulates supply chain issue  \n",
    "3. **Marketing Budget Cut** (40% reduction) - Simulates business decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GENERATE ANOMALOUS DATA - SCENARIO 1: Server Capacity Failure\n",
    "# ============================================================================\n",
    "\n",
    "anomaly_config_1 = {\n",
    "    'node': 'server_capacity',\n",
    "    'start_idx': 600,\n",
    "    'end_idx': 800,\n",
    "    'effect': 0.5,  # 50% reduction\n",
    "    'description': 'Server capacity reduced by 50% due to infrastructure failure'\n",
    "}\n",
    "\n",
    "df_anomaly_1, anomaly_info_1 = generate_ecommerce_data(\n",
    "    n_samples=800,\n",
    "    inject_anomaly=True,\n",
    "    anomaly_config=anomaly_config_1\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SCENARIO 1: SERVER CAPACITY FAILURE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Root cause: {anomaly_info_1['node']}\")\n",
    "print(f\"Description: {anomaly_info_1['description']}\")\n",
    "print(f\"Anomaly period: Samples {anomaly_info_1['start_idx']}-{anomaly_info_1['end_idx']}\")\n",
    "print(f\"Original mean: {anomaly_info_1['original_mean']:.2f}\")\n",
    "print(f\"Anomaly mean: {anomaly_info_1['anomaly_mean']:.2f}\")\n",
    "print(f\"Percentage change: {anomaly_info_1['pct_change']:.1f}%\")\n",
    "\n",
    "# Extract anomaly period\n",
    "df_anomaly_period_1 = df_anomaly_1.iloc[600:800].copy()\n",
    "print(f\"\\nAnomalous period shape: {df_anomaly_period_1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GENERATE ANOMALOUS DATA - SCENARIO 2: Inventory Shortage\n",
    "# ============================================================================\n",
    "\n",
    "anomaly_config_2 = {\n",
    "    'node': 'inventory_level',\n",
    "    'start_idx': 600,\n",
    "    'end_idx': 800,\n",
    "    'effect': 0.7,  # 30% reduction\n",
    "    'description': 'Inventory reduced by 30% due to supply chain disruption'\n",
    "}\n",
    "\n",
    "df_anomaly_2, anomaly_info_2 = generate_ecommerce_data(\n",
    "    n_samples=800,\n",
    "    inject_anomaly=True,\n",
    "    anomaly_config=anomaly_config_2\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SCENARIO 2: INVENTORY SHORTAGE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Root cause: {anomaly_info_2['node']}\")\n",
    "print(f\"Description: {anomaly_info_2['description']}\")\n",
    "print(f\"Anomaly period: Samples {anomaly_info_2['start_idx']}-{anomaly_info_2['end_idx']}\")\n",
    "print(f\"Original mean: {anomaly_info_2['original_mean']:.2f}\")\n",
    "print(f\"Anomaly mean: {anomaly_info_2['anomaly_mean']:.2f}\")\n",
    "print(f\"Percentage change: {anomaly_info_2['pct_change']:.1f}%\")\n",
    "\n",
    "df_anomaly_period_2 = df_anomaly_2.iloc[600:800].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GENERATE ANOMALOUS DATA - SCENARIO 3: Marketing Budget Cut\n",
    "# ============================================================================\n",
    "\n",
    "anomaly_config_3 = {\n",
    "    'node': 'marketing_spend',\n",
    "    'start_idx': 600,\n",
    "    'end_idx': 800,\n",
    "    'effect': 0.6,  # 40% reduction\n",
    "    'description': 'Marketing budget cut by 40% due to cost reduction initiative'\n",
    "}\n",
    "\n",
    "df_anomaly_3, anomaly_info_3 = generate_ecommerce_data(\n",
    "    n_samples=800,\n",
    "    inject_anomaly=True,\n",
    "    anomaly_config=anomaly_config_3\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SCENARIO 3: MARKETING BUDGET CUT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Root cause: {anomaly_info_3['node']}\")\n",
    "print(f\"Description: {anomaly_info_3['description']}\")\n",
    "print(f\"Anomaly period: Samples {anomaly_info_3['start_idx']}-{anomaly_info_3['end_idx']}\")\n",
    "print(f\"Original mean: {anomaly_info_3['original_mean']:.2f}\")\n",
    "print(f\"Anomaly mean: {anomaly_info_3['anomaly_mean']:.2f}\")\n",
    "print(f\"Percentage change: {anomaly_info_3['pct_change']:.1f}%\")\n",
    "\n",
    "df_anomaly_period_3 = df_anomaly_3.iloc[600:800].copy()\n",
    "\n",
    "print(\"\\n✅ All anomaly scenarios generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Anomaly Propagation\n",
    "\n",
    "Let's visualize how anomalies at root causes cascade through the causal graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZE ANOMALY IMPACT - SCENARIO 1\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, node in enumerate(all_nodes):\n",
    "    # Plot time series\n",
    "    axes[idx].plot(df_anomaly_1['time_period'], df_anomaly_1[node], \n",
    "                   alpha=0.6, label='With Anomaly', linewidth=1.5)\n",
    "    \n",
    "    # Highlight anomaly period\n",
    "    axes[idx].axvspan(600, 800, alpha=0.2, color='red', label='Anomaly Period')\n",
    "    \n",
    "    # Add mean line\n",
    "    normal_mean = df_normal[node].mean()\n",
    "    axes[idx].axhline(normal_mean, color='green', linestyle='--', \n",
    "                     alpha=0.7, label=f'Normal Mean: {normal_mean:.1f}')\n",
    "    \n",
    "    # Calculate percentage drop in anomaly period\n",
    "    anomaly_mean = df_anomaly_1[node].iloc[600:800].mean()\n",
    "    pct_change = ((anomaly_mean - normal_mean) / normal_mean) * 100\n",
    "    \n",
    "    axes[idx].set_title(f'{node}\\n(Anomaly period: {pct_change:+.1f}%)', fontsize=10)\n",
    "    axes[idx].set_xlabel('Time Period')\n",
    "    axes[idx].set_ylabel('Value')\n",
    "    axes[idx].legend(fontsize=8, loc='best')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Scenario 1: Server Capacity Failure - Anomaly Propagation', \n",
    "             y=1.02, fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Anomaly propagation visualized for Scenario 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train Models with Auto ML Mode\n",
    "\n",
    "Now we'll train the HT model using **Auto ML mode**, which will:\n",
    "- Test multiple model types for each node (LinearRegression, RandomForest, XGBoost, LightGBM)\n",
    "- Select the best performing model for each node based on R² or accuracy\n",
    "- Store performance metrics for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAIN HT MODEL WITH AUTO ML\n",
    "# ============================================================================\n",
    "\n",
    "# Configure HT with Auto ML mode\n",
    "config = HTConfig(\n",
    "    graph=adj_matrix,\n",
    "    aggregator=\"max\",\n",
    "    root_cause_top_k=5,\n",
    "    model_type=\"AutoML\",  # This activates Auto ML mode\n",
    "    auto_ml=True,\n",
    "    auto_ml_models=[\"LinearRegression\", \"RandomForest\", \"Xgboost\", \"LightGBM\"]\n",
    ")\n",
    "\n",
    "# Create HT instance\n",
    "ht_model = HT(config)\n",
    "\n",
    "# Train on normal data only\n",
    "print(\"Training HT model with Auto ML mode...\\n\")\n",
    "ht_model.train(df_normal[all_nodes], perform_cv=True, verbose_automl=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ TRAINING COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INSPECT AUTO ML RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AUTO ML MODEL SELECTION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if hasattr(ht_model, 'auto_ml_results'):\n",
    "    for node, results in ht_model.auto_ml_results.items():\n",
    "        print(f\"\\n{node}:\")\n",
    "        for res in results:\n",
    "            if res.get('model') is not None:\n",
    "                status = \"✅ SELECTED\" if res['score'] == max(r['score'] for r in results if r.get('model') is not None) else \"  \"\n",
    "                print(f\"  {status} {res['model_name']:20s} | {res['metric_str']}\")\n",
    "            elif 'error' in res:\n",
    "                print(f\"     ❌ {res['model_name']:20s} | Failed: {res['error'][:50]}\")\n",
    "else:\n",
    "    print(\"Auto ML results not available (not stored during training)\")\n",
    "\n",
    "# Check model quality report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL QUALITY REPORT\")\n",
    "print(\"=\"*70)\n",
    "quality_report = ht_model.get_model_quality_report()\n",
    "print(f\"\\nOverall Quality Grade: {quality_report['trust_indicators']['quality_grade']}\")\n",
    "print(f\"Graph Coverage: {quality_report['trust_indicators']['graph_coverage']}%\")\n",
    "print(f\"\\nRegression Performance:\")\n",
    "print(f\"  Mean R²: {quality_report['overall_summary']['regression_performance']['mean_r2']:.4f}\")\n",
    "print(f\"  Median R²: {quality_report['overall_summary']['regression_performance']['median_r2']:.4f}\")\n",
    "print(f\"  Min R²: {quality_report['overall_summary']['regression_performance']['min_r2']:.4f}\")\n",
    "print(f\"  Max R²: {quality_report['overall_summary']['regression_performance']['max_r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Root Cause Detection - Scenario 1 (Server Capacity)\n",
    "\n",
    "**GROUND TRUTH**: The root cause is `server_capacity` (50% reduction)\n",
    "\n",
    "Let's see if the model can detect it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ROOT CAUSE ANALYSIS - SCENARIO 1\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SCENARIO 1: ROOT CAUSE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Ground Truth: {anomaly_info_1['node']} ({anomaly_info_1['description']})\")\n",
    "print(f\"Expected: RCA should identify '{anomaly_info_1['node']}' as top root cause\\n\")\n",
    "\n",
    "# Run RCA on anomalous period\n",
    "rca_results_1 = ht_model.find_root_causes(\n",
    "    df_anomaly_period_1[all_nodes],\n",
    "    anomalous_metrics='revenue',  # We observe revenue drop\n",
    "    return_paths=True,\n",
    "    adjustment=False\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DETECTED ROOT CAUSES (Top 5)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for idx, rc in enumerate(rca_results_1.root_cause_nodes, 1):\n",
    "    is_correct = \"✅ CORRECT!\" if rc['root_cause'] == anomaly_info_1['node'] else \"\"\n",
    "    print(f\"{idx}. {rc['root_cause']:20s} | Score: {rc['score']:8.2f} | Severity: {rc['severity']} {is_correct}\")\n",
    "\n",
    "# Check if ground truth is in top 3\n",
    "top_3_roots = [rc['root_cause'] for rc in rca_results_1.root_cause_nodes[:3]]\n",
    "detection_success_1 = anomaly_info_1['node'] in top_3_roots\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "if detection_success_1:\n",
    "    rank = top_3_roots.index(anomaly_info_1['node']) + 1\n",
    "    print(f\"✅ SUCCESS: Ground truth '{anomaly_info_1['node']}' detected at rank {rank}\")\n",
    "else:\n",
    "    print(f\"❌ FAILURE: Ground truth '{anomaly_info_1['node']}' NOT in top 3\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test Root Cause Detection - Scenario 2 (Inventory)\n",
    "\n",
    "**GROUND TRUTH**: The root cause is `inventory_level` (30% reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ROOT CAUSE ANALYSIS - SCENARIO 2\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SCENARIO 2: ROOT CAUSE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Ground Truth: {anomaly_info_2['node']} ({anomaly_info_2['description']})\")\n",
    "print(f\"Expected: RCA should identify '{anomaly_info_2['node']}' as top root cause\\n\")\n",
    "\n",
    "# Run RCA\n",
    "rca_results_2 = ht_model.find_root_causes(\n",
    "    df_anomaly_period_2[all_nodes],\n",
    "    anomalous_metrics='revenue',\n",
    "    return_paths=True,\n",
    "    adjustment=False\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DETECTED ROOT CAUSES (Top 5)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for idx, rc in enumerate(rca_results_2.root_cause_nodes, 1):\n",
    "    is_correct = \"✅ CORRECT!\" if rc['root_cause'] == anomaly_info_2['node'] else \"\"\n",
    "    print(f\"{idx}. {rc['root_cause']:20s} | Score: {rc['score']:8.2f} | Severity: {rc['severity']} {is_correct}\")\n",
    "\n",
    "top_3_roots = [rc['root_cause'] for rc in rca_results_2.root_cause_nodes[:3]]\n",
    "detection_success_2 = anomaly_info_2['node'] in top_3_roots\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "if detection_success_2:\n",
    "    rank = top_3_roots.index(anomaly_info_2['node']) + 1\n",
    "    print(f\"✅ SUCCESS: Ground truth '{anomaly_info_2['node']}' detected at rank {rank}\")\n",
    "else:\n",
    "    print(f\"❌ FAILURE: Ground truth '{anomaly_info_2['node']}' NOT in top 3\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Test Root Cause Detection - Scenario 3 (Marketing)\n",
    "\n",
    "**GROUND TRUTH**: The root cause is `marketing_spend` (40% reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ROOT CAUSE ANALYSIS - SCENARIO 3\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SCENARIO 3: ROOT CAUSE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Ground Truth: {anomaly_info_3['node']} ({anomaly_info_3['description']})\")\n",
    "print(f\"Expected: RCA should identify '{anomaly_info_3['node']}' as top root cause\\n\")\n",
    "\n",
    "# Run RCA\n",
    "rca_results_3 = ht_model.find_root_causes(\n",
    "    df_anomaly_period_3[all_nodes],\n",
    "    anomalous_metrics='revenue',\n",
    "    return_paths=True,\n",
    "    adjustment=False\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DETECTED ROOT CAUSES (Top 5)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for idx, rc in enumerate(rca_results_3.root_cause_nodes, 1):\n",
    "    is_correct = \"✅ CORRECT!\" if rc['root_cause'] == anomaly_info_3['node'] else \"\"\n",
    "    print(f\"{idx}. {rc['root_cause']:20s} | Score: {rc['score']:8.2f} | Severity: {rc['severity']} {is_correct}\")\n",
    "\n",
    "top_3_roots = [rc['root_cause'] for rc in rca_results_3.root_cause_nodes[:3]]\n",
    "detection_success_3 = anomaly_info_3['node'] in top_3_roots\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "if detection_success_3:\n",
    "    rank = top_3_roots.index(anomaly_info_3['node']) + 1\n",
    "    print(f\"✅ SUCCESS: Ground truth '{anomaly_info_3['node']}' detected at rank {rank}\")\n",
    "else:\n",
    "    print(f\"❌ FAILURE: Ground truth '{anomaly_info_3['node']}' NOT in top 3\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Summary and Evaluation\n",
    "\n",
    "Let's summarize the detection performance across all scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL EVALUATION SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL EVALUATION: ROOT CAUSE DETECTION PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "scenarios = [\n",
    "    {\n",
    "        'name': 'Scenario 1: Server Capacity Failure',\n",
    "        'ground_truth': anomaly_info_1['node'],\n",
    "        'detected_roots': [rc['root_cause'] for rc in rca_results_1.root_cause_nodes[:5]],\n",
    "        'success': detection_success_1\n",
    "    },\n",
    "    {\n",
    "        'name': 'Scenario 2: Inventory Shortage',\n",
    "        'ground_truth': anomaly_info_2['node'],\n",
    "        'detected_roots': [rc['root_cause'] for rc in rca_results_2.root_cause_nodes[:5]],\n",
    "        'success': detection_success_2\n",
    "    },\n",
    "    {\n",
    "        'name': 'Scenario 3: Marketing Budget Cut',\n",
    "        'ground_truth': anomaly_info_3['node'],\n",
    "        'detected_roots': [rc['root_cause'] for rc in rca_results_3.root_cause_nodes[:5]],\n",
    "        'success': detection_success_3\n",
    "    }\n",
    "]\n",
    "\n",
    "successes = 0\n",
    "for scenario in scenarios:\n",
    "    print(f\"\\n{scenario['name']}\")\n",
    "    print(f\"  Ground Truth: {scenario['ground_truth']}\")\n",
    "    print(f\"  Detected (Top 5): {scenario['detected_roots']}\")\n",
    "    \n",
    "    if scenario['ground_truth'] in scenario['detected_roots']:\n",
    "        rank = scenario['detected_roots'].index(scenario['ground_truth']) + 1\n",
    "        print(f\"  Result: ✅ DETECTED at rank {rank}\")\n",
    "        successes += 1\n",
    "    else:\n",
    "        print(f\"  Result: ❌ NOT DETECTED in top 5\")\n",
    "\n",
    "accuracy = (successes / len(scenarios)) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"OVERALL ACCURACY: {successes}/{len(scenarios)} ({accuracy:.1f}%)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if accuracy >= 66.7:  # At least 2 out of 3\n",
    "    print(\"\\n✅ SYSTEM VALIDATION: PASSED\")\n",
    "    print(\"The root cause analysis system successfully identified most injected anomalies!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ SYSTEM VALIDATION: NEEDS IMPROVEMENT\")\n",
    "    print(\"The system struggled to identify the injected anomalies.\")\n",
    "    print(\"Consider: (1) More training data, (2) Better model selection, (3) Reviewing causal structure\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NOTEBOOK COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Auto ML Mode**: The system automatically tested multiple model types (LinearRegression, RandomForest, XGBoost, LightGBM) for each node and selected the best performer\n",
    "\n",
    "2. **Controlled Testing**: By injecting known anomalies, we can objectively evaluate the detection system's performance\n",
    "\n",
    "3. **Causal Propagation**: Anomalies naturally cascade through the causal graph, affecting downstream nodes - our synthetic data correctly simulates this\n",
    "\n",
    "4. **Detection Accuracy**: The system's ability to identify root causes depends on:\n",
    "   - Quality of causal graph structure\n",
    "   - Model accuracy for each node\n",
    "   - Strength of the anomaly signal\n",
    "   - Amount of training data\n",
    "\n",
    "5. **Practical Application**: This approach can be used to:\n",
    "   - Validate the RCA system before production deployment\n",
    "   - Test different causal graph structures\n",
    "   - Benchmark different model types\n",
    "   - Understand detection limitations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
