{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retail Store Sales Optimization\n",
    "\n",
    "**Goal**: Find optimal interventions to increase store sales by 20%\n",
    "\n",
    "This notebook demonstrates how to use the Intervention Search system to identify the best ways to improve retail store performance through causal interventions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load retail data\n",
    "df = pd.read_csv('data/retail_data.csv')\n",
    "print(f\"Loaded {len(df)} retail stores\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Causal Graph\n",
    "\n",
    "**Causal Structure:**\n",
    "- `store_location → foot_traffic → sales`\n",
    "- `store_size → inventory_level → sales`\n",
    "- `marketing_spend → foot_traffic`\n",
    "- `price_discount → conversion_rate → sales`\n",
    "- `staff_count → customer_satisfaction → sales`\n",
    "- `competitor_proximity → foot_traffic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define causal graph as adjacency matrix\n",
    "nodes = ['store_location', 'store_size', 'marketing_spend', 'price_discount', \n",
    "         'staff_count', 'competitor_proximity', 'foot_traffic', 'inventory_level', \n",
    "         'conversion_rate', 'customer_satisfaction', 'sales']\n",
    "\n",
    "edges = [\n",
    "    ('store_location', 'foot_traffic'),\n",
    "    ('marketing_spend', 'foot_traffic'),\n",
    "    ('competitor_proximity', 'foot_traffic'),\n",
    "    ('store_size', 'inventory_level'),\n",
    "    ('price_discount', 'conversion_rate'),\n",
    "    ('staff_count', 'customer_satisfaction'),\n",
    "    ('foot_traffic', 'sales'),\n",
    "    ('inventory_level', 'sales'),\n",
    "    ('conversion_rate', 'sales'),\n",
    "    ('customer_satisfaction', 'sales')\n",
    "]\n",
    "\n",
    "# Create adjacency matrix\n",
    "adj_matrix = pd.DataFrame(0, index=nodes, columns=nodes)\n",
    "for parent, child in edges:\n",
    "    adj_matrix.loc[parent, child] = 1\n",
    "\n",
    "print(\"Causal Graph:\")\n",
    "print(adj_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Causal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ht_categ import HT, HTConfig\n",
    "\n",
    "# Create and train HT model\n",
    "config = HTConfig(graph=adj_matrix, model_type='XGBoost')\n",
    "ht_model = HT(config)\n",
    "ht_model.train(df)\n",
    "\n",
    "print(\"✓ Causal model trained\")\n",
    "print(f\"\\nModel metrics (R² scores):\")\n",
    "for node, metrics in ht_model.model_metrics.items():\n",
    "    if 'r2' in metrics:\n",
    "        print(f\"  {node}: {metrics['r2']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Find Optimal Interventions\n",
    "\n",
    "**Objective**: Increase sales by 20% with high confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intervention_search import InterventionSearch\n",
    "\n",
    "# Initialize intervention search\n",
    "searcher = InterventionSearch(\n",
    "    graph=ht_model.graph,\n",
    "    ht_model=ht_model,\n",
    "    n_simulations=1000\n",
    ")\n",
    "\n",
    "# Search for interventions to increase sales by 20%\n",
    "results = searcher.find_interventions(\n",
    "    target_outcome='sales',\n",
    "    target_change=20.0,  # +20% increase\n",
    "    tolerance=3.0,       # ±3% tolerance\n",
    "    confidence_level=0.90,\n",
    "    max_intervention_pct=30.0,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Best Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = results['best_intervention']\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMENDED INTERVENTION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nIntervene on: {', '.join(best['nodes'])}\")\n",
    "print(f\"\\nRequired changes:\")\n",
    "for node, change in best['required_pct_changes'].items():\n",
    "    baseline = ht_model.baseline_stats[node]['mean']\n",
    "    new_value = baseline * (1 + change/100)\n",
    "    print(f\"  • {node}: {change:+.1f}% (from {baseline:.0f} to {new_value:.0f})\")\n",
    "\n",
    "print(f\"\\nExpected Impact:\")\n",
    "print(f\"  • Predicted sales change: {best['actual_effect']:+.1f}% (target: +20.0%)\")\n",
    "print(f\"  • 90% Confidence Interval: [{best['ci_90'][0]:+.1f}%, {best['ci_90'][1]:+.1f}%]\")\n",
    "print(f\"  • 50% Confidence Interval: [{best['ci_50'][0]:+.1f}%, {best['ci_50'][1]:+.1f}%]\")\n",
    "print(f\"  • Confidence Score: {best['confidence']:.0%}\")\n",
    "print(f\"  • Status: {'✅ APPROVED' if best['within_tolerance'] else '❌ NOT APPROVED'}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Top Interventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 5 interventions\n",
    "print(\"\\nTop 5 Interventions:\\n\")\n",
    "for i, candidate in enumerate(results['all_candidates'][:5], 1):\n",
    "    print(f\"{i}. {', '.join(candidate['nodes'])}\")\n",
    "    print(f\"   Effect: {candidate['actual_effect']:+.1f}% | \"\n",
    "          f\"Confidence: {candidate['confidence']:.0%} | \"\n",
    "          f\"Quality: {candidate['quality']['overall_grade']}\")\n",
    "    print(f\"   Changes: {candidate['required_pct_changes']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Path Analysis\n",
    "\n",
    "Understanding which causal paths contribute most to the effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if 'path_analysis' in results and results['path_analysis'] is not None:\n    path_info = results['path_analysis']\n    print(\"\\n\" + \"=\"*70)\n    print(\"CAUSAL PATH SENSITIVITY ANALYSIS (RCA)\")\n    print(\"=\"*70)\n    \n    if 'path_contributions' in path_info:\n        print(\"\\nPath Contributions to Total Effect:\")\n        for i, path_contrib in enumerate(path_info['path_contributions'][:5], 1):\n            path_str = path_contrib.get('path', 'Unknown')\n            contribution_pct = path_contrib.get('contribution_pct', 0)\n            quality = path_contrib.get('quality_score', 0)\n            print(f\"{i}. {path_str}\")\n            print(f\"   Contribution: {contribution_pct:.1f}%\")\n            print(f\"   Path Quality: {quality:.2f}\")\n            print()\n    \n    if 'total_paths' in path_info:\n        print(f\"Total Causal Paths: {path_info['total_paths']}\")\n        print(f\"High Quality Paths (>0.7): {path_info.get('high_quality_paths', 'N/A')}\")\n        print(f\"Average Path Quality: {path_info.get('avg_path_quality', 0):.3f}\")\n        \nelse:\n    print(\"\\nPath analysis not available for this intervention.\")\n    print(\"Using direct path decomposition instead...\")\n    \n    # Manual path analysis\n    from intervention_search.core.path_analyzer import PathSensitivityAnalyzer\n    \n    best = results['best_intervention']\n    intervention_node = best['nodes'][0]\n    target_node = 'sales'\n    \n    analyzer = PathSensitivityAnalyzer(\n        graph=ht_model.graph,\n        model_metrics=ht_model.model_metrics,\n        edge_elasticities=getattr(ht_model, 'edge_elasticities', {})\n    )\n    \n    path_analysis = analyzer.decompose_total_effect(\n        intervention_node,\n        target_node,\n        best['actual_effect']\n    )\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"ROOT CAUSE ANALYSIS - Path Decomposition\")\n    print(\"=\"*70)\n    print(f\"\\nIntervention: {intervention_node} → {target_node}\")\n    print(f\"Total Effect: {best['actual_effect']:+.1f}%\")\n    print(f\"\\nDirect vs. Indirect Effects:\")\n    print(f\"  • Direct Effect: {path_analysis.get('direct_effect', 0):.1f}%\")\n    print(f\"  • Indirect Effect: {path_analysis.get('indirect_effect', 0):.1f}%\")\n    print(f\"  • Number of Paths: {path_analysis.get('num_paths', 0)}\")\n    \n    if 'path_contributions' in path_analysis:\n        print(f\"\\nTop Contributing Paths:\")\n        for i, path in enumerate(path_analysis['path_contributions'][:3], 1):\n            print(f\"  {i}. {path.get('path', 'N/A')} ({path.get('contribution_pct', 0):.1f}% of effect)\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Model Quality Assessment\n\nUnderstanding the reliability of predictions through quality grading",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Model Quality Summary\nquality_report = results.get('quality_report', {})\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"MODEL QUALITY REPORT\")\nprint(\"=\"*70)\n\nprint(f\"\\nOverall Grade: {quality_report.get('overall_grade', 'N/A')}\")\nprint(f\"Total Models: {quality_report.get('total_models', 0)}\")\nprint(f\"Regression Models: {quality_report.get('regression_models', 0)}\")\nprint(f\"Mean R²: {quality_report.get('mean_r2', 0):.3f}\")\n\nif 'grade_distribution' in quality_report:\n    print(f\"\\nGrade Distribution:\")\n    for grade in ['A', 'B', 'C', 'D', 'F']:\n        count = quality_report['grade_distribution'].get(grade, 0)\n        if count > 0:\n            print(f\"  Grade {grade}: {count} models\")\n\n# Best intervention quality\nbest = results['best_intervention']\nif 'quality' in best:\n    quality = best['quality']\n    print(f\"\\n{'-'*70}\")\n    print(\"Best Intervention Quality:\")\n    print(f\"  Path: {quality.get('path', 'N/A')}\")\n    print(f\"  Overall Grade: {quality.get('quality_grade', 'N/A')}\")\n    print(f\"  Geometric Mean Quality: {quality.get('quality_score_geom_mean', 0):.3f}\")\n    print(f\"  Weakest Link: {quality.get('weakest_link', {}).get('node', 'N/A')} \"\n          f\"(Grade: {quality.get('weakest_link', {}).get('grade', 'N/A')})\")\n    \n    if quality.get('warnings'):\n        print(f\"\\n  ⚠️  Warnings:\")\n        for warning in quality['warnings'][:3]:\n            print(f\"    - {warning}\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Business Interpretation\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **Primary Levers**: The analysis identifies which operational variables have the strongest causal impact on sales\n",
    "2. **Confidence Levels**: High confidence scores indicate reliable predictions based on strong model quality\n",
    "3. **Uncertainty**: Confidence intervals account for model uncertainty through Monte Carlo simulation\n",
    "4. **Feasibility**: Interventions are validated for out-of-distribution detection and practical constraints\n",
    "\n",
    "### Recommended Actions:\n",
    "\n",
    "Based on the best intervention identified:\n",
    "- Implement the recommended changes gradually\n",
    "- Monitor actual vs. predicted outcomes\n",
    "- Consider multi-node interventions for robust improvements\n",
    "- Focus on high-quality causal paths for maximum reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- ✅ Loading and preparing retail data\n",
    "- ✅ Defining causal graph structure\n",
    "- ✅ Training causal models with HT\n",
    "- ✅ Finding optimal interventions with uncertainty quantification\n",
    "- ✅ Analyzing causal paths and model quality\n",
    "- ✅ Interpreting results for business decisions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}