{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retail Store Sales Optimization\n",
    "\n",
    "**Goal**: Find optimal interventions to increase store sales by 20%\n",
    "\n",
    "This notebook demonstrates how to use the Intervention Search system to identify the best ways to improve retail store performance through causal interventions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load retail data\n",
    "df = pd.read_csv('data/retail_data.csv')\n",
    "print(f\"Loaded {len(df)} retail stores\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Causal Graph\n",
    "\n",
    "**Causal Structure:**\n",
    "- `store_location → foot_traffic → sales`\n",
    "- `store_size → inventory_level → sales`\n",
    "- `marketing_spend → foot_traffic`\n",
    "- `price_discount → conversion_rate → sales`\n",
    "- `staff_count → customer_satisfaction → sales`\n",
    "- `competitor_proximity → foot_traffic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define causal graph as adjacency matrix\n",
    "nodes = ['store_location', 'store_size', 'marketing_spend', 'price_discount', \n",
    "         'staff_count', 'competitor_proximity', 'foot_traffic', 'inventory_level', \n",
    "         'conversion_rate', 'customer_satisfaction', 'sales']\n",
    "\n",
    "edges = [\n",
    "    ('store_location', 'foot_traffic'),\n",
    "    ('marketing_spend', 'foot_traffic'),\n",
    "    ('competitor_proximity', 'foot_traffic'),\n",
    "    ('store_size', 'inventory_level'),\n",
    "    ('price_discount', 'conversion_rate'),\n",
    "    ('staff_count', 'customer_satisfaction'),\n",
    "    ('foot_traffic', 'sales'),\n",
    "    ('inventory_level', 'sales'),\n",
    "    ('conversion_rate', 'sales'),\n",
    "    ('customer_satisfaction', 'sales')\n",
    "]\n",
    "\n",
    "# Create adjacency matrix\n",
    "adj_matrix = pd.DataFrame(0, index=nodes, columns=nodes)\n",
    "for parent, child in edges:\n",
    "    adj_matrix.loc[parent, child] = 1\n",
    "\n",
    "print(\"Causal Graph:\")\n",
    "print(adj_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Causal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ht_categ import HT, HTConfig\n",
    "\n",
    "# Create and train HT model\n",
    "config = HTConfig(graph=adj_matrix, model_type='XGBoost')\n",
    "ht_model = HT(config)\n",
    "ht_model.train(df)\n",
    "\n",
    "print(\"✓ Causal model trained\")\n",
    "print(f\"\\nModel metrics (R² scores):\")\n",
    "for node, metrics in ht_model.model_metrics.items():\n",
    "    if 'r2' in metrics:\n",
    "        print(f\"  {node}: {metrics['r2']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Find Optimal Interventions\n",
    "\n",
    "**Objective**: Increase sales by 20% with high confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intervention_search import InterventionSearch\n",
    "\n",
    "# Initialize intervention search\n",
    "searcher = InterventionSearch(\n",
    "    graph=ht_model.graph,\n",
    "    ht_model=ht_model,\n",
    "    n_simulations=1000\n",
    ")\n",
    "\n",
    "# Search for interventions to increase sales by 20%\n",
    "results = searcher.find_interventions(\n",
    "    target_outcome='sales',\n",
    "    target_change=20.0,  # +20% increase\n",
    "    tolerance=3.0,       # ±3% tolerance\n",
    "    confidence_level=0.90,\n",
    "    max_intervention_pct=30.0,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Best Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = results['best_intervention']\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMENDED INTERVENTION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nIntervene on: {', '.join(best['nodes'])}\")\n",
    "print(f\"\\nRequired changes:\")\n",
    "for node, change in best['required_pct_changes'].items():\n",
    "    baseline = ht_model.baseline_stats[node]['mean']\n",
    "    new_value = baseline * (1 + change/100)\n",
    "    print(f\"  • {node}: {change:+.1f}% (from {baseline:.0f} to {new_value:.0f})\")\n",
    "\n",
    "print(f\"\\nExpected Impact:\")\n",
    "print(f\"  • Predicted sales change: {best['actual_effect']:+.1f}% (target: +20.0%)\")\n",
    "print(f\"  • 90% Confidence Interval: [{best['ci_90'][0]:+.1f}%, {best['ci_90'][1]:+.1f}%]\")\n",
    "print(f\"  • 50% Confidence Interval: [{best['ci_50'][0]:+.1f}%, {best['ci_50'][1]:+.1f}%]\")\n",
    "print(f\"  • Confidence Score: {best['confidence']:.0%}\")\n",
    "print(f\"  • Status: {'✅ APPROVED' if best['within_tolerance'] else '❌ NOT APPROVED'}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Top Interventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 5 interventions\n",
    "print(\"\\nTop 5 Interventions:\\n\")\n",
    "for i, candidate in enumerate(results['all_candidates'][:5], 1):\n",
    "    print(f\"{i}. {', '.join(candidate['nodes'])}\")\n",
    "    print(f\"   Effect: {candidate['actual_effect']:+.1f}% | \"\n",
    "          f\"Confidence: {candidate['confidence']:.0%} | \"\n",
    "          f\"Quality: {candidate['quality']['overall_grade']}\")\n",
    "    print(f\"   Changes: {candidate['required_pct_changes']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Path Analysis\n",
    "\n",
    "Understanding which causal paths contribute most to the effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'path_analysis' in results:\n",
    "    path_info = results['path_analysis']\n",
    "    print(\"\\nCausal Path Sensitivity Analysis:\")\n",
    "    print(f\"  • Total paths analyzed: {path_info.get('total_paths', 'N/A')}\")\n",
    "    print(f\"  • High quality paths: {path_info.get('high_quality_paths', 'N/A')}\")\n",
    "    print(f\"  • Average path quality: {path_info.get('avg_path_quality', 0):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Business Interpretation\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **Primary Levers**: The analysis identifies which operational variables have the strongest causal impact on sales\n",
    "2. **Confidence Levels**: High confidence scores indicate reliable predictions based on strong model quality\n",
    "3. **Uncertainty**: Confidence intervals account for model uncertainty through Monte Carlo simulation\n",
    "4. **Feasibility**: Interventions are validated for out-of-distribution detection and practical constraints\n",
    "\n",
    "### Recommended Actions:\n",
    "\n",
    "Based on the best intervention identified:\n",
    "- Implement the recommended changes gradually\n",
    "- Monitor actual vs. predicted outcomes\n",
    "- Consider multi-node interventions for robust improvements\n",
    "- Focus on high-quality causal paths for maximum reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- ✅ Loading and preparing retail data\n",
    "- ✅ Defining causal graph structure\n",
    "- ✅ Training causal models with HT\n",
    "- ✅ Finding optimal interventions with uncertainty quantification\n",
    "- ✅ Analyzing causal paths and model quality\n",
    "- ✅ Interpreting results for business decisions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
